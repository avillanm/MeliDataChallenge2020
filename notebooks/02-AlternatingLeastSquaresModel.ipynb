{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de factorización de matrices: Alternating Least Squares\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "Alternating Least Squares es un modelo de recomendación de factorización de matrices con ratings implícitos. La ventaja que tiene este modelo sobre otros, es que la representación implícita no se representa de manera binaria, sino que permite codificar un nivel de confianza sobre la interacción entre usuarios e ítems. Según el [paper del modelo](http://yifanhu.net/PUB/cf.pdf), la confianza *cui* de que un usuario *u* tenga interés en un ítem *i*, está dada por la siguiente ecuación.\n",
    "\n",
    "\\begin{equation*}\n",
    "c_{ui} = 1 + α r_{ui}\n",
    "\\end{equation*}\n",
    "\n",
    "Donde α es una constante (hiperparámetro del modelo), que empíricamente se recomienda setear en 40.\n",
    "\n",
    "rui son los ratings implícitos que se utilizarán. Se busca que este valor sea mayor cuando exista una mayor señal de interacción del usuario sobre un ítem. Así que viene genial para utilizar como rui la fórmula ya encontrada para pesar los ítems según cantidad de pageviews y la posiciones temporales de estas visitas. Recapitulando, si se toman las posiciones de todas las visitas en una sesión sobre un ítem, dónde la posición 1 es la posición más cercana al la compra, se puede formar un rating con esta fórmula:\n",
    "\n",
    "\\begin{equation*}\n",
    "r_{ui} = \\sum_{pos} \\frac{1}{log_{10}(pos + 1)}\n",
    "\\end{equation*}\n",
    "\n",
    "Lo interesante es que de esta forma se codifica la temporalidad en las sesiones. Otro enfoque interesante fue codificar a las ventas con un rui bastante mayor a los referidos a las visitas, así de esta manera, tendrán mayor peso sobre la función loss.\n",
    "\n",
    "Otro enfoque adicional fueron codificar atributos de los ítems (dominios, países, términos de títulos y precios), como también los términos usados en las búsquedas, como si fuesen ítems adicionales. También en este caso se utiliza el score que relaciona cantidad de pageviews y la temporalidad, pero con un peso menor que los ítems, cuestión de indicarle al modelo que ponga más foco sobre los ítems. La idea de este enfoque es agregar semántica al entrenamiento, sobre todo para ítems con pocas interacciones; y que se suela recomendar productos de un mismo dominio, siendo esto último muy importante para la métrica de evaluación usada.\n",
    "\n",
    "\n",
    "### Resultados\n",
    "\n",
    "Se entrenaron dos modelos, con un conjunto de atributos distintos. En el primer modelo se formó, usando adicionalmente a los ítems: dominios, precios, país y términos de búsquedas. El **ndcg** en test fue **0.3082**.\n",
    "\n",
    "El segundo modelo se utilizaron, en adición a los ítems, los términos de los títulos de las publicaciones. En este caso el **ndcg** en test fue **0.2981**.\n",
    "\n",
    "Luego se ensamblaran estos resultados para generar una mejor predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " #import os\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "from scipy import sparse\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import pickle\n",
    "import implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapea ítems con dominios, y país del dominio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_TO_DOMAIN = {}\n",
    "with open(\"./data/item_data.jl\", \"rt\") as fd:\n",
    "    for line in fd:\n",
    "        data = json.loads(line)\n",
    "        ITEM_TO_DOMAIN[data[\"item_id\"]] = data[\"domain_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_TO_COUNTRY = {}\n",
    "with open(\"./data/item_data.jl\", \"rt\") as fd:\n",
    "    for line in fd:\n",
    "        data = json.loads(line)\n",
    "        ITEM_TO_COUNTRY[data[\"item_id\"]] =data[\"category_id\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métrica de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDCG = np.sum([(1 if i != 1 else 12) / np.log2(1 + i) for i in range(1, 11)])\n",
    "\n",
    "def dcg(rec, y_item_id, n=10):\n",
    "    y_domain = ITEM_TO_DOMAIN[y_item_id]\n",
    "    \n",
    "    return np.sum([(1 if yhat_item_id != y_item_id else 12) / np.log2(1 + i)\\\n",
    "                   for i, yhat_item_id in enumerate(rec[:n], 1)\\\n",
    "                  if (ITEM_TO_DOMAIN[yhat_item_id] == y_domain)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexa ítems\n",
    "\n",
    "Se toman para el modelo ítems con visitas en al menos dos sesiones o que tengan compras en test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_counter = Counter()\n",
    "bought_counter = Counter()\n",
    "\n",
    "# datos de entrenamiento\n",
    "with open(\"./data/train_dataset-train_split.jl\", \"rt\") as fd:\n",
    "    for line in fd:\n",
    "        data = json.loads(line)\n",
    "\n",
    "        item_bought = data[\"item_bought\"]\n",
    "        items_views = set([event[\"event_info\"] for event in data[\"user_history\"] if event[\"event_type\"] == \"view\"])\n",
    "        views_counter.update(items_views)\n",
    "        bought_counter[item_bought] += 1\n",
    "\n",
    "# datos de test\n",
    "with open(\"./data/train_dataset-test_split.jl\", \"rt\") as fd:\n",
    "    for line in fd:\n",
    "        data = json.loads(line)\n",
    "        item_bought = data[\"item_bought\"]\n",
    "        items_views = set([event[\"event_info\"] for event in data[\"user_history\"] if event[\"event_type\"] == \"view\"])\n",
    "        views_counter.update(items_views)\n",
    "        \n",
    "# datos de validación\n",
    "with open(\"./data/train_dataset-val_split.jl\", \"rt\") as fd:\n",
    "    for line in fd:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "        except:\n",
    "            continue\n",
    "        item_bought = data[\"item_bought\"]\n",
    "        items_views = set([event[\"event_info\"] for event in data[\"user_history\"] if event[\"event_type\"] == \"view\"])\n",
    "        views_counter.update(items_views)\n",
    "\n",
    "# datos de submission\n",
    "with open(\"./data/test_dataset.jl\", \"rt\") as fd:\n",
    "    for line in fd:\n",
    "        data = json.loads(line)\n",
    "        items_views = set([event[\"event_info\"] for event in data[\"user_history\"] if event[\"event_type\"] == \"view\"])\n",
    "        views_counter.update(items_views)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de artículos en sesiones  2,098,660\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de artículos en sesiones {len(views_counter): ,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de artículos con más de una sesión  536,257\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de artículos con más de una sesión {np.sum([v > 1 for v in  views_counter.values() ]): ,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de artículos al menos 3 sesiones  285,976\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de artículos al menos 3 sesiones {np.sum([v >= 3 for v in  views_counter.values() ]): ,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "minsup = 2\n",
    "\n",
    "items = set([k for k, c in  views_counter.items() if c >= minsup]) | set(bought_counter.keys())\n",
    "\n",
    "ITEM_TO_IDX = {item_id: idx  for idx, item_id in enumerate(items)}\n",
    "IDX_TO_ITEM = {idx: item_id   for item_id, idx  in ITEM_TO_IDX.items()}\n",
    "\n",
    "del items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = len(ITEM_TO_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "542583"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexa Dominios\n",
    "Se indexan los dominos para incorporar a los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominios de catálogo: 7,893\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dominios de catálogo: {len(set([domain_id for domain_id in ITEM_TO_DOMAIN.values() if domain_id])):,d}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datos de entrenamiento\n",
    "domain_counter = Counter()\n",
    "with open(\"./data/train_dataset-train_split.jl\", \"rt\") as fd:\n",
    "    for line in fd:\n",
    "        data = json.loads(line)\n",
    "\n",
    "        item_bought = data[\"item_bought\"]\n",
    "        items_views = set([event[\"event_info\"] for event in data[\"user_history\"] if event[\"event_type\"] == \"view\"])\n",
    "        items_views.add(item_bought)\n",
    "        domains = set([ITEM_TO_DOMAIN[item_id] for item_id in items_views if ITEM_TO_DOMAIN[item_id]])\n",
    "        \n",
    "        domain_counter.update(domains)\n",
    "\n",
    "# datos de test\n",
    "with open(\"./data/train_dataset-test_split.jl\", \"rt\") as fd:\n",
    "    for line in fd:\n",
    "        data = json.loads(line)\n",
    "\n",
    "        item_bought = data[\"item_bought\"]\n",
    "        items_views = set([event[\"event_info\"] for event in data[\"user_history\"] if event[\"event_type\"] == \"view\"])\n",
    "        domains = set([ITEM_TO_DOMAIN[item_id] for item_id in items_views if ITEM_TO_DOMAIN[item_id]])\n",
    "        \n",
    "        domain_counter.update(domains)\n",
    "\n",
    "# datos de validacion\n",
    "with open(\"./data/train_dataset-val_split.jl\", \"rt\") as fd:\n",
    "    for line in fd:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        item_bought = data[\"item_bought\"]\n",
    "        items_views = set([event[\"event_info\"] for event in data[\"user_history\"] if event[\"event_type\"] == \"view\"])\n",
    "        domains = set([ITEM_TO_DOMAIN[item_id] for item_id in items_views if ITEM_TO_DOMAIN[item_id]])\n",
    "        \n",
    "        domain_counter.update(domains)\n",
    "\n",
    "# datos de submission\n",
    "with open(\"./data/test_dataset.jl\", \"rt\") as fd:\n",
    "    for line in fd:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "        items_views = set([event[\"event_info\"] for event in data[\"user_history\"] if event[\"event_type\"] == \"view\"])\n",
    "        domains = set([ITEM_TO_DOMAIN[item_id] for item_id in items_views if ITEM_TO_DOMAIN[item_id]])\n",
    "        \n",
    "        domain_counter.update(domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominios en prodoductos con sesiones: 7,893\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dominios en prodoductos con sesiones: {len(domain_counter):,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de categorías con una única sesión  527\n",
      "Cantidad de categorías con al menos 2 sesiones  7,366\n",
      "Cantidad de categorías con al menos 3 sesiones  6,952\n",
      "Cantidad de categorías con al menos 4 sesiones  6,651\n",
      "Cantidad de categorías con al menos 5 sesiones  6,376\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de categorías con una única sesión {np.sum([v == 1 for v in  domain_counter.values() ]): ,d}\")\n",
    "print(f\"Cantidad de categorías con al menos 2 sesiones {np.sum([v >= 2 for v in  domain_counter.values() ]): ,d}\")\n",
    "print(f\"Cantidad de categorías con al menos 3 sesiones {np.sum([v >= 3 for v in  domain_counter.values() ]): ,d}\")\n",
    "print(f\"Cantidad de categorías con al menos 4 sesiones {np.sum([v >= 4 for v in  domain_counter.values() ]): ,d}\")\n",
    "print(f\"Cantidad de categorías con al menos 5 sesiones {np.sum([v >= 5 for v in  domain_counter.values() ]): ,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "minsup = 2\n",
    "\n",
    "domains = [k for k, c in  domain_counter.most_common() if c >= minsup]\n",
    "\n",
    "DOMAIN_TO_IDX = {domain_id: idx  for idx, domain_id in enumerate(domains)}\n",
    "IDX_TO_DOMAIN = {idx: domain_id   for domain_id, idx  in DOMAIN_TO_IDX.items()}\n",
    "\n",
    "del domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7366"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DOMAIN_TO_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atributos Items: Precios\n",
    "Se definen bins para discretizar los precios y así utilizarlos como atributos adicionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRICE_ITEM = {}\n",
    "with open(\"./data/item_data.jl\", \"rt\") as fd:\n",
    "    for line in fd:\n",
    "        data = json.loads(line)\n",
    "        PRICE_ITEM[data[\"item_id\"]] = float(data[\"price\"]) if data[\"price\"] else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999999999.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([price for price in PRICE_ITEM.values() if price >= 1 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRICES_BINS = [0, 5]\n",
    "for i in range(1, 10):\n",
    "    price_break = 10**(i)\n",
    "    PRICES_BINS.append(price_break)\n",
    "    if  1 < i <= 4:\n",
    "        PRICES_BINS.extend([price_break + 10**(i) * j for j in range(1, 9)])\n",
    "    else:\n",
    "        PRICES_BINS.append(price_break + 10**(i) * 4 )\n",
    "PRICES_BINS.append(10000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 10, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000, 500000, 1000000, 5000000, 10000000, 50000000, 100000000, 500000000, 1000000000, 5000000000, 10000000000]\n"
     ]
    }
   ],
   "source": [
    "print(PRICES_BINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_rng_counter = Counter(np.digitize([price for price in PRICE_ITEM.values() if price], PRICES_BINS) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 420631), (3, 361576), (2, 358583), (5, 192863), (13, 131698), (6, 115995), (7, 77586), (8, 58147), (14, 48000), (9, 46190), (10, 36773), (11, 30750), (12, 28936), (22, 25489), (15, 25097), (31, 22827), (16, 15405), (23, 15391), (24, 12032), (17, 10601), (1, 8921), (25, 8142), (18, 7566), (19, 6451), (26, 5801), (20, 5052), (21, 4617), (27, 4473), (28, 3620), (29, 2880), (32, 2861), (30, 2659), (0, 1942), (33, 1658), (35, 256), (34, 248), (37, 62), (39, 60), (38, 52), (36, 39), (40, 8)]\n"
     ]
    }
   ],
   "source": [
    "print(prices_rng_counter.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atributos Items: Títulos de publicaciones\n",
    "\n",
    "Se seleccionan términos en los títulos de las publicaciones. Para la selección de los términos se utilizaron reglas de asociación de tamaño 2 entre los términos (antecedente) y los dominios (consecuente). De esta forma se busca elegir términos con mayor contenido semántico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import unidecode\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_norm(text):\n",
    "    return unidecode.unidecode(text).lower()\n",
    "\n",
    "stop_words = stopwords.words('portuguese') + stopwords.words('spanish')\n",
    "stop_words =  set([text_norm(w) for w in stop_words])\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w{2,}')\n",
    "def tokenize(text):\n",
    "    return [w for w in tokenizer.tokenize(text_norm(text)) if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True para levantar datos precalculados\n",
    "LOAD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD:\n",
    "    # cuenta frecuencias de dominios o tokens\n",
    "    domain_count = Counter()\n",
    "    word_count = Counter()\n",
    "    N = 0\n",
    "    with open(\"./data/item_data.jl\", \"rt\") as fd:\n",
    "        for line in fd:\n",
    "            data = json.loads(line)\n",
    "            if data[\"domain_id\"]:\n",
    "                domain_count[data[\"domain_id\"]] += 1\n",
    "                word_count.update(set(tokenize(data[\"title\"])))\n",
    "                N += 1\n",
    "    # selecciona dominios o tokens por soporte mínimo\n",
    "    min_sup = 5\n",
    "    domain_count = Counter({k: c for k, c in domain_count.items() if c >= min_sup})\n",
    "    word_count = Counter({k: c for k, c in word_count.items() if c >= min_sup})\n",
    "    \n",
    "    # Itemsets de tamaño 2\n",
    "    domain_word_count = Counter()\n",
    "    with open(\"./data/item_data.jl\", \"rt\") as fd:\n",
    "        for line in fd:\n",
    "            data = json.loads(line)\n",
    "            if data[\"domain_id\"] in domain_count:\n",
    "                domain_word_count.update([(data[\"domain_id\"], word)\\\n",
    "                                         for word in set(tokenize(data[\"title\"])) if word in word_count])\n",
    "            \n",
    "    # calcula métricas de reglas de asociación\n",
    "    df_asso = pd.DataFrame([[d, w, c, (c / word_count[w]), (c * N / (word_count[w] * domain_count[d]) if domain_count[d] else 0)]\\\n",
    "                        for (d, w), c  in domain_word_count.items()],\n",
    "                        columns=[\"domain_id\", \"word\", \"sup_count\", \"conf\", \"lift\"])\n",
    "\n",
    "    # selecciona asociaciones por métricas\n",
    "    df_asso = df_asso[(df_asso.lift > 1.1) & (df_asso.conf >= 0.2) & (df_asso.sup_count >= min_sup) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asociaciones más fuertes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain_id</th>\n",
       "      <th>word</th>\n",
       "      <th>sup_count</th>\n",
       "      <th>conf</th>\n",
       "      <th>lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127266</th>\n",
       "      <td>MLM-CROWBARS</td>\n",
       "      <td>barreta</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>350237.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352389</th>\n",
       "      <td>MLB-TENNIS_AND_SQUASH_RACKET_VIBRATION_DAMPENERS</td>\n",
       "      <td>antivibrador</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>350237.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374873</th>\n",
       "      <td>MLM-GINS</td>\n",
       "      <td>ginebra</td>\n",
       "      <td>5</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>300203.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998404</th>\n",
       "      <td>MLB-ROCK_CRUSHERS</td>\n",
       "      <td>britador</td>\n",
       "      <td>6</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>300203.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126847</th>\n",
       "      <td>MLM-PICKGUARDS</td>\n",
       "      <td>pickguard</td>\n",
       "      <td>5</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>300203.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173867</th>\n",
       "      <td>MLM-IGNITION_KNOCK_SENSORS</td>\n",
       "      <td>detonacion</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>291864.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187904</th>\n",
       "      <td>MLB-ANALOG_RAIN_GAUGES</td>\n",
       "      <td>pluviometro</td>\n",
       "      <td>5</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>262678.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139737</th>\n",
       "      <td>MLM-TAMBOURINES</td>\n",
       "      <td>pandero</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>233491.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598697</th>\n",
       "      <td>MLB-KAZOOS</td>\n",
       "      <td>kazoo</td>\n",
       "      <td>5</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>233491.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178202</th>\n",
       "      <td>MLM-EDGE_BANDING_MACHINES</td>\n",
       "      <td>enchapadora</td>\n",
       "      <td>7</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>233491.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                domain_id          word  \\\n",
       "127266                                       MLM-CROWBARS       barreta   \n",
       "1352389  MLB-TENNIS_AND_SQUASH_RACKET_VIBRATION_DAMPENERS  antivibrador   \n",
       "374873                                           MLM-GINS       ginebra   \n",
       "998404                                  MLB-ROCK_CRUSHERS      britador   \n",
       "126847                                     MLM-PICKGUARDS     pickguard   \n",
       "173867                         MLM-IGNITION_KNOCK_SENSORS    detonacion   \n",
       "1187904                            MLB-ANALOG_RAIN_GAUGES   pluviometro   \n",
       "139737                                    MLM-TAMBOURINES       pandero   \n",
       "598697                                         MLB-KAZOOS         kazoo   \n",
       "178202                          MLM-EDGE_BANDING_MACHINES   enchapadora   \n",
       "\n",
       "         sup_count      conf           lift  \n",
       "127266           5  0.833333  350237.666667  \n",
       "1352389          6  1.000000  350237.666667  \n",
       "374873           5  0.714286  300203.714286  \n",
       "998404           6  0.857143  300203.714286  \n",
       "126847           5  0.714286  300203.714286  \n",
       "173867           5  0.833333  291864.722222  \n",
       "1187904          5  0.625000  262678.250000  \n",
       "139737           6  1.000000  233491.777778  \n",
       "598697           5  0.555556  233491.777778  \n",
       "178202           7  0.777778  233491.777778  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not LOAD:\n",
    "    from IPython.display import display\n",
    "    display(df_asso.sort_values(\"lift\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexa los términos seleccionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD:\n",
    "    # palabras seleccionadas\n",
    "    items_tokens = df_asso.word.unique()\n",
    "\n",
    "    # indexa palabras/tokens\n",
    "    ITEM_TOKEN_TO_IDX = {w: idx for idx, w in enumerate(items_tokens)}\n",
    "    IDX_TO_ITEM_TOKEN = {idx: feat for feat, idx  in ITEM_TOKEN_TO_IDX.items()}\n",
    "\n",
    "    # calcula document frecuency del token\n",
    "    DF_ITEM_TOKEN = {ITEM_TOKEN_TO_IDX[word]: word_count[word] for word in items_tokens}\n",
    "\n",
    "    ITEM_TO_TOKENS_IDX = {}\n",
    "    with open(\"./data/item_data.jl\", \"rt\") as fd:\n",
    "        for line in fd:\n",
    "            data = json.loads(line)        \n",
    "            ITEM_TO_TOKENS_IDX[data[\"item_id\"]] =\\\n",
    "                [ITEM_TOKEN_TO_IDX[w] for w in set(tokenize(data[\"title\"])) if w in ITEM_TOKEN_TO_IDX]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guarda o levanta índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD == False:\n",
    "    var = {\n",
    "        \"ITEM_TO_TOKENS_IDX\": ITEM_TO_TOKENS_IDX,\n",
    "        \"ITEM_TOKEN_TO_IDX\": ITEM_TOKEN_TO_IDX,\n",
    "        \"IDX_TO_ITEM_TOKEN\": IDX_TO_ITEM_TOKEN,\n",
    "        \"DF_ITEM_TOKEN\": DF_ITEM_TOKEN,\n",
    "    }\n",
    "    \n",
    "    with open(\"data/models/items_title_tokens.pkl\", \"wb\") as fd:\n",
    "        pickle.dump(var, fd, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(\"data/models/items_title_tokens.pkl\", \"rb\") as fd:\n",
    "        var = pickle.load(fd)\n",
    "        ITEM_TO_TOKENS_IDX = var[\"ITEM_TO_TOKENS_IDX\"] \n",
    "        ITEM_TOKEN_TO_IDX = var[\"ITEM_TOKEN_TO_IDX\"]\n",
    "        IDX_TO_ITEM_TOKEN = var[\"IDX_TO_ITEM_TOKEN\"]\n",
    "        DF_ITEM_TOKEN = var[\"DF_ITEM_TOKEN\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexa búsquedas\n",
    "Se indexan términos (o tokens) de utilizados en búsquedas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_counter = Counter()\n",
    "n_users = 0\n",
    "# datos de entrenamiento\n",
    "with open(\"./data/train_dataset-train_split.jl\", \"rt\") as fd:\n",
    "    for line in fd:\n",
    "        data = json.loads(line)\n",
    "        searchs = set([event[\"event_info\"].upper().strip() for event in data[\"user_history\"] if event[\"event_type\"] == \"search\"])\n",
    "        search_counter.update(searchs)\n",
    "        n_users += 1\n",
    "        \n",
    "# datos de test\n",
    "with open(\"./data/train_dataset-test_split.jl\", \"rt\") as fd:\n",
    "    for line in fd:\n",
    "        data = json.loads(line)\n",
    "        searchs = set([event[\"event_info\"].upper().strip() for event in data[\"user_history\"] if event[\"event_type\"] == \"search\"])\n",
    "        search_counter.update(searchs)\n",
    "        n_users += 1\n",
    "        \n",
    "# datos de val\n",
    "with open(\"./data/train_dataset-val_split.jl\", \"rt\") as fd:\n",
    "    for line in fd:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "        except:\n",
    "            continue\n",
    "        searchs = set([event[\"event_info\"].upper().strip() for event in data[\"user_history\"] if event[\"event_type\"] == \"search\"])\n",
    "        search_counter.update(searchs)\n",
    "        n_users += 1\n",
    "        \n",
    "# datos de submission\n",
    "with open(\"./data/test_dataset.jl\", \"rt\") as fd:\n",
    "    for line in fd:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "        except:\n",
    "            continue\n",
    "        searchs = set([event[\"event_info\"].upper().strip() for event in data[\"user_history\"] if event[\"event_type\"] == \"search\"])\n",
    "        search_counter.update(searchs)\n",
    "        n_users += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de búsquedas únicas en sesiones  1,157,186\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de búsquedas únicas en sesiones {len(search_counter): ,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de búsquedas con más de una sesión  158,894\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de búsquedas con más de una sesión {np.sum([v > 1 for v in  search_counter.values() ]): ,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de búsquedas con al menos 3 sesiones  80,584\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de búsquedas con al menos 3 sesiones {np.sum([v >= 3 for v in  search_counter.values() ]): ,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuenta tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter()\n",
    "for text in search_counter.keys():\n",
    "    token_counter.update(set(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tokens únicas en búsquedas  143,370\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de tokens únicas en búsquedas {len(token_counter): ,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tokens en más de una búsqueda  74,968\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de tokens en más de una búsqueda {np.sum([v > 1 for v in  token_counter.values() ]): ,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tokens en al menos 3 búsquedas  54,712\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de tokens en al menos 3 búsquedas {np.sum([v >= 3 for v in  token_counter.values() ]): ,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tokens en al menos 5 búsquedas  38,245\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de tokens en al menos 5 búsquedas {np.sum([v >= 5 for v in  token_counter.values() ]): ,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecciona tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "minsup = 5\n",
    "\n",
    "searchs = set([k for k, c in  search_counter.items() if c >= minsup and len(k) >= 3])\n",
    "\n",
    "minsup = 5\n",
    "tokens = set([k for k, c in  token_counter.items() if c >= minsup and len(k) >= 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_search_pop = set([ token for text in searchs for token in text.split() if len(token) >=3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37152, 12263, 37698)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens), len(tokens_search_pop), len(tokens | tokens_search_pop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens | tokens_search_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_TOKEN_TO_IDX = {feat: idx  for idx, feat in enumerate(tokens)}\n",
    "IDX_TO_SEARCH_TOKEN = {idx: feat for feat, idx  in SEARCH_TOKEN_TO_IDX.items()}\n",
    "DF_SEARCH_TOKEN = {feat: token_counter[feat]  for feat in tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz utilidad\n",
    "\n",
    "La matriz de utilidad relaciona a los usuarios o sesiones (filas), con los ítems visitados o comprados (columnas). Adicionalmente se codifican como si fuesen otros ítems (columnas), los atributos derivados de los ítems de la sesión (categoría, país, precios, tokens de título), cómo también los términos usados en las búsquedas.\n",
    "\n",
    "Esta representación de matriz es implícita, pero en vez de utilizar una representación binaria, se utiliza un score para pesar la confianza que se tiene en la interacción user/ítem. Este score tiene en cuenta la cantidad de visitas en la sesión sobre el ítem, el órden temporal de esas visitas, y si el ítem fue finalmente comprado (esto último para datos de test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: 542,583 - Dominios: 7,366 - Paises: 2 - Precios: 42 - Title tokens: 48,804 - Search Tokens: 37,698\n"
     ]
    }
   ],
   "source": [
    "print(f\"Items: {len(ITEM_TO_IDX):,d} - \"\n",
    "      f\"Dominios: {len(DOMAIN_TO_IDX):,d} - \"\n",
    "      f\"Paises: 2 - \"\n",
    "      f\"Precios: {len(PRICES_BINS)} - \"\n",
    "      f\"Title tokens: {len(ITEM_TOKEN_TO_IDX):,d} - \"\n",
    "      f\"Search Tokens: {len(SEARCH_TOKEN_TO_IDX):,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interactions(user_idx, items, item_weight):\n",
    "    \"\"\"\n",
    "    Genera coordenadas de matriz rala para representar las interacciones.\n",
    "    \n",
    "    @param user_idx: Índice de fila\n",
    "    @param items: Listado de ítems visitados o comprados (en formato de id de catálogo)\n",
    "    @param item_weight: Pesos o scores del ítem en la sesión.\n",
    "    \n",
    "    return Tulpas de dimensión 5 con las siguientes valores:\n",
    "     - Posición 0: Coordenadas y valores de score de iteracciones con ítems. (Listado de tuplas de tamaño 3: (user_idx, item_idx, score))\n",
    "     - Posición 1: Coordenadas de iteracciones con dominios. (mismo formato que ítems)\n",
    "     - Posición 2: Coordenadas de interacciones con países (mismo formato que ítems)\n",
    "     - Posición 3: Coordenadas de intenteracciones con bins de precios. (mismo formato que ítems)\n",
    "     - Posición 4: Coordenadas de intenteracciones tokens de títulos. (mismo formato que ítems)\n",
    "    \"\"\"\n",
    "    global ITEM_TO_IDX, ITEM_TO_DOMAIN, PRICE_ITEM, DOMAIN_TO_IDX, ITEM_TO_COUNTRY, PRICES_BINS,\\\n",
    "           DF_ITEM_TOKEN, ITEM_TO_TOKENS_IDX\n",
    "    \n",
    "    # items\n",
    "    inte_items = [(user_idx, ITEM_TO_IDX[item_id], weight)\\\n",
    "                  for item_id, weight in zip(items, item_weight) if item_id in ITEM_TO_IDX]\n",
    "\n",
    "    # dominios\n",
    "    domain_weight = {}\n",
    "    for item_id, weight in zip(items, item_weight):\n",
    "        domain_id = ITEM_TO_DOMAIN[item_id] \n",
    "        if domain_id in DOMAIN_TO_IDX:\n",
    "            domain_idx = DOMAIN_TO_IDX[domain_id] \n",
    "            domain_weight[domain_idx] = domain_weight.get(domain_idx, 0) + weight\n",
    "    inte_domains = [(user_idx, domain_ix, weight / len(items) ) for domain_ix, weight in domain_weight.items()]\n",
    "\n",
    "    # paises\n",
    "    inte_counties = [(user_idx, 0 if c == \"B\" else 1, 1)  for c in set([ITEM_TO_COUNTRY[item_id] for item_id in items])]\n",
    "\n",
    "    # precios\n",
    "    price_weight = {}\n",
    "    for item_id, weight in zip(items, item_weight):\n",
    "        price = PRICE_ITEM[item_id] \n",
    "        if price:\n",
    "            price_idx = np.digitize(price, PRICES_BINS) - 1\n",
    "            price_weight[price_idx] = price_weight.get(price_idx, 0) + weight\n",
    "    inte_prices = [(user_idx, price_idx, weight  / len(items) ) for price_idx, weight in price_weight.items()]\n",
    "    \n",
    "    # title\n",
    "    token_weight = {}\n",
    "    for item_id, weight in zip(items, item_weight):\n",
    "        tokens_idx = ITEM_TO_TOKENS_IDX[item_id] \n",
    "        for token_idx in tokens_idx:\n",
    "            token_weight[token_idx] = token_weight.get(token_idx, 0) + weight\n",
    "    inte_tokens = [(user_idx, token_idx, tf/ DF_ITEM_TOKEN[token_idx]) for token_idx, tf in token_weight.items()]\n",
    "            \n",
    "    return inte_items, inte_domains, inte_counties, inte_prices, inte_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_interactions(user_idx, texts, weights):\n",
    "    \"\"\"\n",
    "    Genera coordenadas de matriz rala para representar las interacciones de los términos de busquedas.\n",
    "    \n",
    "    @param user_idx: Índice de fila\n",
    "    @param texts: Texto de búsqueda\n",
    "    @param weights: Pesos o scores según posición en la sesión\n",
    "    \n",
    "    return  Coordenadas y valores de score de iteracciones con términos. (Listado de tuplas de tamaño 3: (user_idx, token, score))\n",
    "    \"\"\"\n",
    "    global SEARCH_TOKEN_TO_IDX, DF_SEARCH_TOKEN\n",
    "    \n",
    "    token_tf = {}\n",
    "    for text, w in zip(texts, weights):\n",
    "            \n",
    "        for token in set(text.split()):\n",
    "            if token in SEARCH_TOKEN_TO_IDX:\n",
    "                token_tf[token] = token_tf.get(token, 0) + w\n",
    "    inte_tokens = [(user_idx, SEARCH_TOKEN_TO_IDX[token], tf/ DF_SEARCH_TOKEN[token]) for token, tf in token_tf.items()]\n",
    "    \n",
    "    return inte_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iteractions(path, user_idx = 0, is_train=False):\n",
    "    \"\"\"\n",
    "    Genera índices y valores de matriz.\n",
    "    \"\"\"\n",
    "    interaction_items = []\n",
    "    interactions_domains = []\n",
    "    interactions_country = []\n",
    "    interactions_prices = []\n",
    "    interactions_item_token = []\n",
    "    interactions_search_token = []\n",
    "    y = []\n",
    "    BOUGHT_WEIGHT= 30\n",
    "\n",
    "    with open(path, \"rt\") as fd:\n",
    "        for line in fd:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            item_bought = data.get(\"item_bought\")\n",
    "            y.append(item_bought)\n",
    "            \n",
    "            events = data[\"user_history\"][::-1]\n",
    "            items_views = [event[\"event_info\"] for event in events if event[\"event_type\"] == \"view\"]\n",
    "            searchs = [event[\"event_info\"].upper().strip() for event in events if event[\"event_type\"] == \"search\"]\n",
    "\n",
    "            # Ranking de items visitados\n",
    "            items_pv_count = {}\n",
    "            for pos, item_view in enumerate(items_views, 1):\n",
    "                items_pv_count[item_view] = items_pv_count.get(item_view, 0) + 1 / np.log10(pos + 1)\n",
    "            if is_train:\n",
    "                items_pv_count[item_bought] = BOUGHT_WEIGHT\n",
    "            items, item_weight = zip(*items_pv_count.items()) if items_pv_count else ([], [])\n",
    "\n",
    "            # ranking de busquedas\n",
    "            search_weights = {}\n",
    "            for pos, text in enumerate(searchs, 1):\n",
    "                search_weights[text] = search_weights.get(text, 0) + 1 / np.log10(pos + 1)\n",
    "            texts, search_weights = zip(*search_weights.items()) if search_weights else ([], [])\n",
    "\n",
    "            # índices items/atributos\n",
    "            inte_i, inte_d, inte_c, inte_p, inte_itkn = get_interactions(user_idx, items, item_weight)\n",
    "            interactions_items.extend(inte_i)\n",
    "            interactions_domains.extend(inte_d)\n",
    "            interactions_country.extend(inte_c)\n",
    "            interactions_prices.extend(inte_p)\n",
    "            interactions_item_token.extend(inte_itkn)\n",
    "\n",
    "            # pindices búquedas\n",
    "            inte_stkn  = get_search_interactions(user_idx, texts, search_weights)\n",
    "            interactions_search_token.extend(inte_stkn)\n",
    "            user_idx += 1\n",
    "    return (user_idx, y, interactions_items, interactions_domains, interactions_country,\n",
    "            interactions_prices, interactions_item_token, interactions_search_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtiene iteracciones en fromato de lista de tuplas (row, column, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_items = []\n",
    "interactions_domains = []\n",
    "interactions_country = []\n",
    "interactions_prices = []\n",
    "interactions_item_token = []\n",
    "interactions_search_token = []\n",
    "\n",
    "# train\n",
    "user_idx = 0 \n",
    "iteractions = read_iteractions(\"./data/train_dataset-train_split.jl\", user_idx, True)\n",
    "user_idx = iteractions[0]\n",
    "interactions_items.extend(iteractions[2])\n",
    "interactions_domains.extend(iteractions[3])\n",
    "interactions_country.extend(iteractions[4])\n",
    "interactions_prices.extend(iteractions[5])\n",
    "interactions_item_token.extend(iteractions[6])\n",
    "interactions_search_token.extend(iteractions[7])\n",
    "\n",
    "# test\n",
    "test_ini_idx = user_idx\n",
    "iteractions = read_iteractions(\"./data/train_dataset-test_split.jl\", user_idx, False)\n",
    "user_idx = iteractions[0]\n",
    "y_test = iteractions[1]\n",
    "interactions_items.extend(iteractions[2])\n",
    "interactions_domains.extend(iteractions[3])\n",
    "interactions_country.extend(iteractions[4])\n",
    "interactions_prices.extend(iteractions[5])\n",
    "interactions_item_token.extend(iteractions[6])\n",
    "interactions_search_token.extend(iteractions[7])\n",
    "\n",
    "\n",
    "# val\n",
    "val_ini_idx = user_idx\n",
    "iteractions = read_iteractions(\"./data/train_dataset-val_split.jl\", user_idx, False)\n",
    "user_idx = iteractions[0]\n",
    "y_val = iteractions[1]\n",
    "interactions_items.extend(iteractions[2])\n",
    "interactions_domains.extend(iteractions[3])\n",
    "interactions_country.extend(iteractions[4])\n",
    "interactions_prices.extend(iteractions[5])\n",
    "interactions_item_token.extend(iteractions[6])\n",
    "interactions_search_token.extend(iteractions[7])\n",
    "\n",
    "# datos de submission\n",
    "sub_ini_idx = user_idx\n",
    "iteractions = read_iteractions(\"./data/test_dataset.jl\", user_idx, False)\n",
    "user_idx = iteractions[0]\n",
    "interactions_items.extend(iteractions[2])\n",
    "interactions_domains.extend(iteractions[3])\n",
    "interactions_country.extend(iteractions[4])\n",
    "interactions_prices.extend(iteractions[5])\n",
    "interactions_item_token.extend(iteractions[6])\n",
    "interactions_search_token.extend(iteractions[7])\n",
    "\n",
    "\n",
    "del iteractions\n",
    "n_users = user_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<590231x542583 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3508641 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = user_idx\n",
    "row, col, data  = zip(*interactions_items)\n",
    "ui_matrix = sparse.coo_matrix((data, (row, col)), shape = (n_users, len(ITEM_TO_IDX)) ).tocsr()\n",
    "ui_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<590231x7366 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2078509 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = user_idx\n",
    "row, col, data  = zip(*interactions_domains)\n",
    "ud_matrix = sparse.coo_matrix((data, (row, col)), shape = (n_users, len(DOMAIN_TO_IDX)) ).tocsr()\n",
    "ud_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<590231x2 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 576281 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = user_idx\n",
    "row, col, data  = zip(*interactions_country)\n",
    "uc_matrix = sparse.coo_matrix((data, (row, col)), shape = (n_users, 2) ).tocsr()\n",
    "uc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<590231x41 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2058834 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = user_idx\n",
    "row, col, data  = zip(*interactions_prices)\n",
    "uprices_matrix = sparse.coo_matrix((data, (row, col)) ).tocsr()\n",
    "uprices_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<590231x48804 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10221767 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = user_idx\n",
    "row, col, data  = zip(*interactions_item_token)\n",
    "uitoken_matrix = sparse.coo_matrix((data, (row, col))).tocsr()\n",
    "uitoken_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<590231x37698 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3740406 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = user_idx\n",
    "row, col, data  = zip(*interactions_search_token)\n",
    "ustoken_matrix = sparse.coo_matrix((data, (row, col))).tocsr()\n",
    "ustoken_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valida que todas las columnas tengan valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ui_matrix.sum(axis=0) == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ud_matrix.sum(axis=0) == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(uc_matrix.sum(axis=0) == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(uprices_matrix.sum(axis=0) == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(uitoken_matrix.sum(axis=0) == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ustoken_matrix.sum(axis=0) == 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices en pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD == False:\n",
    "    var = {\n",
    "        # matrices\n",
    "        \"ui_matrix\": ui_matrix,\n",
    "        \"ud_matrix\": ud_matrix,\n",
    "        \"uc_matrix\": uc_matrix,\n",
    "        \"uprices_matrix\": uprices_matrix,\n",
    "        \"uitoken_matrix\": uitoken_matrix,\n",
    "        \"ustoken_matrix\": ustoken_matrix,\n",
    "        \n",
    "        # índices\n",
    "        \"ITEM_TO_IDX\": ITEM_TO_IDX,\n",
    "        \"DOMAIN_TO_IDX\": DOMAIN_TO_IDX,\n",
    "        \"IDX_TO_ITEM\": IDX_TO_ITEM,\n",
    "        \"IDX_TO_DOMAIN\":IDX_TO_DOMAIN,\n",
    "\n",
    "        # índices de conjuntos de test y etiquetas\n",
    "        \"test_ini_idx\": test_ini_idx,\n",
    "        \"y_test\": y_test,\n",
    "        \"val_ini_idx\": val_ini_idx,\n",
    "        \"y_val\": y_val,\n",
    "        \"sub_ini_idx\": sub_ini_idx,\n",
    "    }\n",
    "    \n",
    "    with open(\"data/models/implicit_matrix_variables.pkl\", \"wb\") as fd:\n",
    "        pickle.dump(var, fd, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(\"data/models/implicit_matrix_variables.pkl\", \"rb\") as fd:\n",
    "        var = pickle.load(fd)\n",
    "        ui_matrix = var[\"ui_matrix\"]\n",
    "        ud_matrix = var[\"ud_matrix\"]\n",
    "        uc_matrix = var[\"uc_matrix\"]\n",
    "        uprices_matrix = var[\"uprices_matrix\"]\n",
    "        uitoken_matrix = var[\"uitoken_matrix\"]\n",
    "        ustoken_matrix = var[\"ustoken_matrix\"]\n",
    "        \n",
    "        DOMAIN_TO_IDX = var[\"DOMAIN_TO_IDX\"]\n",
    "        IDX_TO_DOMAIN = var[\"IDX_TO_DOMAIN\"]\n",
    "        \n",
    "        \n",
    "        ITEM_TO_IDX = var[\"ITEM_TO_IDX\"]\n",
    "        DOMAIN_TO_IDX =  var[\"DOMAIN_TO_IDX\"]\n",
    "        IDX_TO_ITEM = var[\"IDX_TO_ITEM\"]\n",
    "        IDX_TO_DOMAIN = var[\"IDX_TO_DOMAIN\"]\n",
    "        \n",
    "        test_ini_idx = var[\"test_ini_idx\"]\n",
    "        y_test = var[\"y_test\"]\n",
    "        val_ini_idx = var[\"val_ini_idx\"]\n",
    "        y_val = var[\"y_val\"]\n",
    "        sub_ini_idx = var[\"sub_ini_idx\"]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de modelos\n",
    "\n",
    "Se entrenaron dos modelos con distintos conjuntos de atributos:\n",
    "\n",
    "modelo 1: ítems + dominios + países + precios + tokens de busqueda\n",
    "\n",
    "modelo 2: ítems + tokens de títulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtiene cantidad de ítems a recomendar\n",
    "n_items = uitems_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normaliza matriz de precios (por filas)\n",
    "up_row_sum = uprices_matrix.sum(axis=1).A.ravel()\n",
    "c = sparse.diags(1/np.where(up_row_sum > 0, up_row_sum, 1 ))\n",
    "uprices_matrix = c.dot(uprices_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_matrix_model1 = sparse.hstack([uitems_matrix, uc_matrix, ud_matrix * 0.5, uprices_matrix , ustoken_matrix ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de una selección de hiperparámetros se entrena este modelo formamdo embebidos de dimensión 1024 (hasta donde dió la memoria, ya que con más resultó mejor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = implicit.als.AlternatingLeastSquares(factors=1024,\n",
    "                                              regularization=0.4, iterations=5, \n",
    "                                              num_threads=4, calculate_training_loss=False, \n",
    "                                              random_state=123)\n",
    "\n",
    "alpha_val = 40\n",
    "data_conf = (ui_matrix_model1.T * alpha_val)\n",
    "model1.fit(data_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación de modelo en test. (guarda las primeras 50 recomendaciones para ensamble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluación en test\n",
    "n_recs = 0\n",
    "sum_dcg = 0\n",
    "\n",
    "# las recomendacione sólo se hacen sobre productos (no queremos ni atributos, ní búsquedas)\n",
    "all_items_idx = list(range(n_items, ui_matrix_model1.shape[1]))\n",
    "\n",
    "# índice de fila inicial de conjunto de test\n",
    "n_train = test_ini_idx\n",
    "\n",
    "test_reco_scores = []\n",
    "for offset, y_item_id in enumerate(y_test):\n",
    "    # scores\n",
    "    recommended = model1.recommend(n_train + offset, ui_matrix_model1,\n",
    "                                  filter_already_liked_items=False,\n",
    "                                   N=50,\n",
    "                                   filter_items=all_items_idx,\n",
    "                                   recalculate_user=False)\n",
    "    test_reco_scores.append([(IDX_TO_ITEM[item_idx], score) for item_idx, score in recommended])\n",
    "    rec = [IDX_TO_ITEM[item_idx] for item_idx, score in recommended][:10]\n",
    "    \n",
    "    # evaluation\n",
    "    sum_dcg += dcg(rec, y_item_id)    \n",
    "    n_recs += 1\n",
    "        \n",
    "print(f\"NDCG: {sum_dcg / (IDCG * n_recs): .4f} - {n_recs} recomendaciones\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NDCG:  0.3082 - 20000 recomendaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_reco_scores = []\n",
    "for offset, y_item_id in enumerate(y_val):\n",
    "    # scores\n",
    "    recommended = model1.recommend(val_ini_idx + offset, ui_matrix_model1,\n",
    "                                  filter_already_liked_items=False, N=50,  filter_items=all_items_idx, recalculate_user=False)\n",
    "    val_reco_scores.append([(IDX_TO_ITEM[item_idx], score) for item_idx, score in recommended])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guarda predicciones para armar ensamble de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_rec = {\n",
    "    \"test_reco_scores\": test_reco_scores,\n",
    "    \"val_reco_scores\": val_reco_scores,\n",
    "}\n",
    "\n",
    "with open(\"data/models/implicit_test_reco_scores_model1.pkl\", \"wb\") as fd:\n",
    "    pickle.dump(var_rec, fd, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtiene cantidad de ítems a recomendar\n",
    "n_items = ui_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciona términos de búsquedas o títulos con al menos 5 sesiones\n",
    "mask = np.asarray(uitoken_matrix.astype(bool).sum(axis=0) >= 5).squeeze()\n",
    "uitoken_matrix = uitoken_matrix[:, mask].tocsr()\n",
    "\n",
    "mask = np.asarray(ustoken_matrix.astype(bool).sum(axis=0) >= 5).squeeze()\n",
    "ustoken_matrix = ustoken_matrix[:, mask].tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_matrix_model2 = sparse.hstack([uitems_matrix, uitoken_matrix, ustoken_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso el recurso escaso de la memoria llegó a cubrir embebidos con 750 dimensiones. Se entrena:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = implicit.als.AlternatingLeastSquares(factors=750,\n",
    "                                              regularization=0.4, iterations=5, \n",
    "                                              num_threads=4, calculate_training_loss=False, \n",
    "                                              random_state=123)\n",
    "\n",
    "alpha_val = 40\n",
    "data_conf = (ui_matrix_model2.T * alpha_val)\n",
    "model2.fit(data_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación en test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluación en test\n",
    "n_recs = 0\n",
    "sum_dcg = 0\n",
    "\n",
    "# las recomendacione sólo se hacen sobre productos (no queremos ni atributos, ní búsquedas)\n",
    "all_items_idx = list(range(n_items, ui_matrix_model2.shape[1]))\n",
    "\n",
    "# índice de fila inicial de conjunto de test\n",
    "n_train = test_ini_idx\n",
    "\n",
    "test_reco_scores = []\n",
    "for offset, y_item_id in enumerate(y_test):\n",
    "    # scores\n",
    "    recommended = model2.recommend(n_train + offset, ui_matrix_model2,\n",
    "                                  filter_already_liked_items=False,\n",
    "                                   N=50,\n",
    "                                   filter_items=all_items_idx,\n",
    "                                   recalculate_user=False)\n",
    "    test_reco_scores.append([(IDX_TO_ITEM[item_idx], score) for item_idx, score in recommended])\n",
    "    rec = [IDX_TO_ITEM[item_idx] for item_idx, score in recommended][:10]\n",
    "    \n",
    "    # evaluation\n",
    "    sum_dcg += dcg(rec, y_item_id)    \n",
    "    n_recs += 1\n",
    "        \n",
    "print(f\"NDCG: {sum_dcg / (IDCG * n_recs): .4f} - {n_recs} recomendaciones\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NDCG:  0.2981 - 2000 recomendaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guarda scores en test y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_reco_scores = []\n",
    "for offset, y_item_id in enumerate(y_val):\n",
    "    # scores\n",
    "    recommended = model2.recommend(val_ini_idx + offset, ui_matrix_model2,\n",
    "                                  filter_already_liked_items=False, N=50,  filter_items=all_items_idx, recalculate_user=False)\n",
    "    val_reco_scores.append([(IDX_TO_ITEM[item_idx], score) for item_idx, score in recommended])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_rec = {\n",
    "    \"test_reco_scores\": test_reco_scores,\n",
    "    \"val_reco_scores\": val_reco_scores,\n",
    "}\n",
    "\n",
    "with open(\"data/models/implicit_test_reco_scores_model2.pkl\", \"wb\") as fd:\n",
    "    pickle.dump(var_rec, fd, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
